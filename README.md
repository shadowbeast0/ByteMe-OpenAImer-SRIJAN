# ByteMe-OpenAImer-SRIJAN

The given Repository consists of the outputs and the link to the notebooks of Track-1 and Track-2 prelims round of OpenAImer SRIJAN

This repository contains outputs and links to notebooks for the prelims round of OpenAImer SRIJAN, covering:

Track-1: Supervised Contrastive Learning

Track-2: ResNet18 Model Compression

Repository content is distilled from our team presentation.

ğŸ“Œ Team & Affiliation

Team ByteMe â€” BCSE UGâ€™27, Jadavpur University
Members: Arjeesh Palai Â· Arko Dasgupta Â· Sombrata Biswas Â· Abhirup Pal

ğŸ” TL;DR
âœ… Track-1 â€” Supervised Contrastive Learning

Pipeline: Two-stage approach

Stage 1: SupCon pre-training (DenseNet201 + MLP projection head)

Stage 2: Linear evaluation (encoder frozen)

Performance:

F1: 0.9948

Accuracy: 0.9936

Precision: 0.9944

Recall: 0.9952

Visualization: t-SNE shows distinct class clusters post contrastive training.

Other backbones tried: ResNet50, InceptionV3, InceptionResNetV2, DenseNet169.

âœ… Track-2 â€” ResNet18 Compression

Approach:

Post-training L1 unstructured weighted pruning (lottery-ticket style).

Architecture slimming: Removed Residual Layers 2â€“4 and Block 2 of Layer 1.

Result:

Disk size: ~0.38 MB

Latency: 8.63 ms

Validation accuracy: 52.8%

Future work: Layer-sensitive pruning, FishLeg/second-order pruning variants.

â“ Why Supervised Contrastive Learning?

Standard cross-entropy struggles when class boundaries blur.

SupCon improves representation by pulling positives (same class) closer and pushing negatives apart using a log-softmax over similarities, scaled by temperature (Ï„ âˆˆ (0,1]).

Result: Compact clusters â†’ better linear separability â†’ improved accuracy with fewer epochs.

ğŸ›  Methodology
Track-1: Architecture & Pipeline

Base encoder: DenseNet201

Projection head: MLP on encoder (SupCon pre-training)

Linear head: Softmax classifier (evaluation stage, encoder frozen)

Training:

Stage 1: Full backprop (encoder + projection head) with SupCon loss

Stage 2: Classifier-only with cross-entropy

Pipeline:

SupCon Pre-training: Image â†’ Encoder â†’ Projection head â†’ SupCon loss

Linear Evaluation: Freeze encoder â†’ Train linear classifier

Track-2: Compression Strategy

Pruning:

L1 unstructured weighted pruning (post-training)

Inspired by lottery-ticket hypothesis

Slimming:

Removed Residual Layers 2â€“4 and Block 2 of Layer 1

Metrics:

Disk size: ~0.38 MB

Latency: 8.63 ms

Validation accuracy: 52.8%
